{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF Signal Detection Project\n",
    "## Spectrogram Processing with Channel/Temporal Slicing for YOLO Training\n",
    "\n",
    "**Project Context:**\n",
    "- Pre-processed spectrograms from Georgia Tech pipeline (256√ó256 RGB)\n",
    "- Center frequency: 2.437 GHz (Wi-Fi Channel 6)\n",
    "- Bandwidth: 20 MHz (captures 2.427 - 2.447 GHz)\n",
    "- Time duration: 410 Œºs per spectrogram frame\n",
    "\n",
    "**Target Signals:**\n",
    "- Bluetooth: 2.402-2.480 GHz (~1 MHz bandwidth, narrow vertical streaks)\n",
    "- Wi-Fi: 2.437 GHz Channel 6 (20 MHz bandwidth, wide rectangular blocks)\n",
    "- Zigbee: 2.405-2.480 GHz (~2 MHz bandwidth, medium width bursts)\n",
    "- Drone: Variable frequency and bandwidth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base (Python 3.11.9)' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q roboflow ultralytics opencv-python matplotlib numpy scikit-learn pyyaml joblib tqdm\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from roboflow import Roboflow\n",
    "from ultralytics import YOLO\n",
    "from IPython.display import display, Image as IPImage\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Configuration & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Configuration\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Central configuration for the RF detection pipeline\"\"\"\n",
    "    \n",
    "    # Hardware settings\n",
    "    WORKERS: int = 8  # Number of parallel workers\n",
    "    N_GPUS: int = 1  # Number of GPUs available\n",
    "    \n",
    "    # Signal parameters\n",
    "    BASE_DURATION_US: float = 410  # Duration per spectrogram frame in microseconds\n",
    "    CENTER_FREQ_GHZ: float = 2.437  # Center frequency in GHz\n",
    "    BANDWIDTH_MHZ: float = 20  # Bandwidth in MHz\n",
    "    \n",
    "    # Bluetooth specific\n",
    "    BLUETOOTH_BANDWIDTH_MHZ: float = 1  # Bluetooth channel bandwidth\n",
    "    BLUETOOTH_SLOT_US: float = 625  # Bluetooth time slot duration\n",
    "    \n",
    "    # Processing parameters\n",
    "    NUM_CHANNELS: int = 4  # Number of frequency channels for slicing\n",
    "    TEMPORAL_OVERLAP: float = 0.75  # Temporal overlap for slicing\n",
    "    \n",
    "    # Dataset paths (will be set after download)\n",
    "    DATASET_PATH: Optional[Path] = None\n",
    "    OUTPUT_DIR: Optional[Path] = None\n",
    "    YOLO_DIR: Optional[Path] = None\n",
    "    \n",
    "    # Training parameters\n",
    "    TRAIN_EPOCHS: int = 100\n",
    "    TRAIN_BATCH_SIZE: int = 16\n",
    "    IMG_SIZE: int = 640\n",
    "    PATIENCE: int = 50\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config()\n",
    "print(\"‚úÖ Configuration initialized\")\n",
    "print(f\"   Workers: {config.WORKERS}\")\n",
    "print(f\"   Center Frequency: {config.CENTER_FREQ_GHZ} GHz\")\n",
    "print(f\"   Bandwidth: {config.BANDWIDTH_MHZ} MHz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Classes & Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BoundingBox:\n",
    "    \"\"\"Represents a bounding box annotation\"\"\"\n",
    "    x: float\n",
    "    y: float\n",
    "    width: float\n",
    "    height: float\n",
    "    category_id: int\n",
    "    category_name: str\n",
    "    confidence: float = 1.0\n",
    "    \n",
    "    def to_yolo_format(self, img_width: int, img_height: int) -> str:\n",
    "        \"\"\"Convert to YOLO format: class_id x_center y_center width height (normalized)\"\"\"\n",
    "        x_center = (self.x + self.width / 2) / img_width\n",
    "        y_center = (self.y + self.height / 2) / img_height\n",
    "        norm_width = self.width / img_width\n",
    "        norm_height = self.height / img_height\n",
    "        return f\"{self.category_id - 1} {x_center} {y_center} {norm_width} {norm_height}\"\n",
    "    \n",
    "    def to_coco_format(self) -> Dict:\n",
    "        \"\"\"Convert to COCO format\"\"\"\n",
    "        return {\n",
    "            'bbox': [self.x, self.y, self.width, self.height],\n",
    "            'category_id': self.category_id,\n",
    "            'area': self.width * self.height\n",
    "        }\n",
    "\n",
    "@dataclass\n",
    "class TimeWindow:\n",
    "    \"\"\"Represents a time window for a spectrogram frame\"\"\"\n",
    "    frame_idx: int\n",
    "    start_us: float\n",
    "    end_us: float\n",
    "    duration_us: float\n",
    "    \n",
    "    def overlaps_with(self, other: 'TimeWindow') -> bool:\n",
    "        \"\"\"Check if this window overlaps with another\"\"\"\n",
    "        return not (self.end_us <= other.start_us or self.start_us >= other.end_us)\n",
    "    \n",
    "    def overlap_percent(self, other: 'TimeWindow') -> float:\n",
    "        \"\"\"Calculate overlap percentage with another window\"\"\"\n",
    "        if not self.overlaps_with(other):\n",
    "            return 0.0\n",
    "        overlap_start = max(self.start_us, other.start_us)\n",
    "        overlap_end = min(self.end_us, other.end_us)\n",
    "        overlap_duration = overlap_end - overlap_start\n",
    "        return 100.0 * overlap_duration / self.duration_us\n",
    "\n",
    "print(\"‚úÖ Data structures defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Dataset Download & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(api_key: str) -> Tuple[Path, Dict]:\n",
    "    \"\"\"\n",
    "    Download dataset from Roboflow and load COCO annotations\n",
    "    \n",
    "    Args:\n",
    "        api_key: Roboflow API key\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (dataset_path, coco_data)\n",
    "    \"\"\"\n",
    "    print(\"üì• Downloading dataset from Roboflow...\")\n",
    "    \n",
    "    rf = Roboflow(api_key=api_key)\n",
    "    project = rf.workspace(\"intelligent-digital-communications\").project(\"ism-band-results-dataset\")\n",
    "    dataset = project.version(49).download(\"coco\")\n",
    "    \n",
    "    dataset_path = Path(dataset.location)\n",
    "    \n",
    "    # Find and load COCO annotations\n",
    "    annotation_files = list(dataset_path.rglob('*.json'))\n",
    "    train_annotations_path = max(annotation_files, key=lambda f: f.stat().st_size)\n",
    "    \n",
    "    with open(train_annotations_path, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "    \n",
    "    print(f\"‚úÖ Dataset downloaded successfully\")\n",
    "    print(f\"   Path: {dataset_path}\")\n",
    "    print(f\"   Images: {len(coco_data['images'])}\")\n",
    "    print(f\"   Annotations: {len(coco_data['annotations'])}\")\n",
    "    print(f\"   Categories: {len(coco_data['categories'])}\")\n",
    "    \n",
    "    # Display categories\n",
    "    for cat in coco_data['categories']:\n",
    "        print(f\"     - {cat['name']} (ID: {cat['id']})\")\n",
    "    \n",
    "    return dataset_path, coco_data\n",
    "\n",
    "# Download dataset\n",
    "ROBOFLOW_API_KEY = \"V74EfwetgJtOmApcRI4g\"  # Replace with your API key\n",
    "config.DATASET_PATH, coco_data = download_dataset(ROBOFLOW_API_KEY)\n",
    "\n",
    "# Set output directories\n",
    "config.OUTPUT_DIR = config.DATASET_PATH.parent / 'dataset_optimized'\n",
    "config.YOLO_DIR = config.DATASET_PATH.parent / 'yolo_final'\n",
    "\n",
    "config.OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "config.YOLO_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Temporal Overlap Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image_pair(img1_path: Path, img2_path: Path) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Analyze temporal overlap between two sequential spectrogram images\n",
    "    \n",
    "    Args:\n",
    "        img1_path: Path to first image\n",
    "        img2_path: Path to second image\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with overlap percentage and correlation, or None if images can't be loaded\n",
    "    \"\"\"\n",
    "    img1 = cv2.imread(str(img1_path), cv2.IMREAD_GRAYSCALE)\n",
    "    img2 = cv2.imread(str(img2_path), cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    if img1 is None or img2 is None:\n",
    "        return None\n",
    "    \n",
    "    width = img1.shape[1]\n",
    "    best_corr = 0\n",
    "    best_overlap = 0\n",
    "    \n",
    "    # Test different overlap percentages\n",
    "    for overlap_pct in range(0, 80, 5):\n",
    "        overlap_width = int(width * overlap_pct / 100)\n",
    "        if overlap_width < 10:\n",
    "            continue\n",
    "        \n",
    "        # Compare right edge of img1 with left edge of img2\n",
    "        right_edge = img1[:, -overlap_width:].flatten()\n",
    "        left_edge = img2[:, :overlap_width].flatten()\n",
    "        \n",
    "        # Calculate correlation\n",
    "        corr = np.corrcoef(right_edge, left_edge)[0, 1]\n",
    "        \n",
    "        if corr > best_corr:\n",
    "            best_corr = corr\n",
    "            best_overlap = overlap_pct\n",
    "    \n",
    "    return {'overlap': best_overlap, 'correlation': best_corr}\n",
    "\n",
    "def detect_temporal_overlap(dataset_path: Path, n_workers: int = 8) -> float:\n",
    "    \"\"\"\n",
    "    Detect temporal overlap across all sequential images in dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to dataset\n",
    "        n_workers: Number of parallel workers\n",
    "        \n",
    "    Returns:\n",
    "        Average overlap percentage detected\n",
    "    \"\"\"\n",
    "    print(\"üîç Analyzing temporal overlap between frames...\")\n",
    "    \n",
    "    # Get all images sorted by name (assumes sequential naming)\n",
    "    all_images = sorted(list(dataset_path.rglob('*.jpg')) + list(dataset_path.rglob('*.png')))\n",
    "    \n",
    "    if len(all_images) < 2:\n",
    "        print(\"‚ö†Ô∏è Not enough images for overlap analysis\")\n",
    "        return 0.0\n",
    "    \n",
    "    # Create pairs of sequential images\n",
    "    pairs = [(all_images[i], all_images[i+1]) for i in range(len(all_images)-1)]\n",
    "    \n",
    "    # Parallel analysis\n",
    "    results = Parallel(n_jobs=n_workers)(\n",
    "        delayed(analyze_image_pair)(p[0], p[1]) for p in tqdm(pairs, desc=\"Analyzing pairs\")\n",
    "    )\n",
    "    \n",
    "    # Filter out None results\n",
    "    results = [r for r in results if r is not None]\n",
    "    \n",
    "    if not results:\n",
    "        print(\"‚ö†Ô∏è No valid overlap results\")\n",
    "        return 0.0\n",
    "    \n",
    "    avg_overlap = np.mean([r['overlap'] for r in results])\n",
    "    avg_correlation = np.mean([r['correlation'] for r in results])\n",
    "    \n",
    "    print(f\"‚úÖ Temporal overlap analysis complete\")\n",
    "    print(f\"   Average overlap: {avg_overlap:.1f}%\")\n",
    "    print(f\"   Average correlation: {avg_correlation:.3f}\")\n",
    "    \n",
    "    return avg_overlap\n",
    "\n",
    "# Detect temporal overlap\n",
    "detected_overlap = detect_temporal_overlap(config.DATASET_PATH, config.WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Sliding Window Signal Continuity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_bluetooth_cutoff(images: List[Path], annotations_dict: Dict, \n",
    "                          time_windows: List[TimeWindow]) -> Dict:\n",
    "    \"\"\"\n",
    "    Check if Bluetooth signals are cut off at image boundaries using sliding window approach\n",
    "    \n",
    "    Args:\n",
    "        images: List of image paths (assumed sequential)\n",
    "        annotations_dict: Dictionary mapping image filename to list of BoundingBox objects\n",
    "        time_windows: List of TimeWindow objects for each frame\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with analysis results\n",
    "    \"\"\"\n",
    "    print(\"\\nüîç Checking for Bluetooth signal cutoffs at image boundaries...\")\n",
    "    \n",
    "    cutoff_detections = []\n",
    "    edge_threshold = 0.05  # 5% from edge is considered \"at boundary\"\n",
    "    \n",
    "    for i in range(len(images) - 1):\n",
    "        img1_name = images[i].name\n",
    "        img2_name = images[i+1].name\n",
    "        \n",
    "        # Get annotations for both images\n",
    "        bboxes1 = annotations_dict.get(img1_name, [])\n",
    "        bboxes2 = annotations_dict.get(img2_name, [])\n",
    "        \n",
    "        # Load images to get dimensions\n",
    "        img1 = cv2.imread(str(images[i]))\n",
    "        if img1 is None:\n",
    "            continue\n",
    "            \n",
    "        img_height, img_width = img1.shape[:2]\n",
    "        \n",
    "        # Check Bluetooth signals near right edge of img1\n",
    "        for bbox in bboxes1:\n",
    "            if 'bluetooth' not in bbox.category_name.lower():\n",
    "                continue\n",
    "            \n",
    "            # Calculate if bbox extends to right edge\n",
    "            right_edge = bbox.x + bbox.width\n",
    "            distance_from_right = img_width - right_edge\n",
    "            \n",
    "            if distance_from_right < edge_threshold * img_width:\n",
    "                # Signal extends to right boundary - check if it continues in next frame\n",
    "                continues = False\n",
    "                \n",
    "                for bbox2 in bboxes2:\n",
    "                    if 'bluetooth' not in bbox2.category_name.lower():\n",
    "                        continue\n",
    "                    \n",
    "                    # Check if bbox2 starts near left edge\n",
    "                    if bbox2.x < edge_threshold * img_width:\n",
    "                        # Check vertical alignment (same frequency channel)\n",
    "                        vertical_overlap = min(bbox.y + bbox.height, bbox2.y + bbox2.height) - max(bbox.y, bbox2.y)\n",
    "                        if vertical_overlap > 0.5 * min(bbox.height, bbox2.height):\n",
    "                            continues = True\n",
    "                            break\n",
    "                \n",
    "                cutoff_detections.append({\n",
    "                    'frame_idx': i,\n",
    "                    'frame1': img1_name,\n",
    "                    'frame2': img2_name,\n",
    "                    'time_window': time_windows[i],\n",
    "                    'bbox': bbox,\n",
    "                    'continues_in_next': continues,\n",
    "                    'distance_from_edge': distance_from_right\n",
    "                })\n",
    "    \n",
    "    # Analyze results\n",
    "    total_detections = len(cutoff_detections)\n",
    "    continuing_signals = sum(1 for d in cutoff_detections if d['continues_in_next'])\n",
    "    cut_signals = total_detections - continuing_signals\n",
    "    \n",
    "    print(f\"\\n‚úÖ Bluetooth cutoff analysis complete:\")\n",
    "    print(f\"   Total Bluetooth signals at boundaries: {total_detections}\")\n",
    "    print(f\"   Signals continuing to next frame: {continuing_signals}\")\n",
    "    print(f\"   Signals potentially cut off: {cut_signals}\")\n",
    "    \n",
    "    if total_detections > 0:\n",
    "        continuity_rate = 100.0 * continuing_signals / total_detections\n",
    "        print(f\"   Continuity rate: {continuity_rate:.1f}%\")\n",
    "    \n",
    "    return {\n",
    "        'detections': cutoff_detections,\n",
    "        'total': total_detections,\n",
    "        'continuing': continuing_signals,\n",
    "        'cut_off': cut_signals\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Bluetooth cutoff detection function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Time Window Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_time_windows(n_images: int, overlap_pct: float, \n",
    "                          base_duration_us: float = 410) -> List[TimeWindow]:\n",
    "    \"\"\"\n",
    "    Calculate absolute time windows for each spectrogram frame\n",
    "    \n",
    "    Args:\n",
    "        n_images: Number of images\n",
    "        overlap_pct: Overlap percentage between frames\n",
    "        base_duration_us: Base duration per frame in microseconds\n",
    "        \n",
    "    Returns:\n",
    "        List of TimeWindow objects\n",
    "    \"\"\"\n",
    "    stride_us = base_duration_us * (1 - overlap_pct / 100)\n",
    "    windows = []\n",
    "    \n",
    "    for i in range(n_images):\n",
    "        start_time = i * stride_us\n",
    "        end_time = start_time + base_duration_us\n",
    "        windows.append(TimeWindow(\n",
    "            frame_idx=i,\n",
    "            start_us=start_time,\n",
    "            end_us=end_time,\n",
    "            duration_us=base_duration_us\n",
    "        ))\n",
    "    \n",
    "    return windows\n",
    "\n",
    "# Get all images\n",
    "all_images = sorted(list(config.DATASET_PATH.rglob('*.jpg')) + \n",
    "                   list(config.DATASET_PATH.rglob('*.png')))\n",
    "\n",
    "# Calculate time windows\n",
    "time_windows = calculate_time_windows(\n",
    "    len(all_images), \n",
    "    detected_overlap, \n",
    "    config.BASE_DURATION_US\n",
    ")\n",
    "\n",
    "total_recording_time_ms = time_windows[-1]['end_us'] / 1000 if time_windows else 0\n",
    "stride_us = config.BASE_DURATION_US * (1 - detected_overlap/100)\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è Temporal partitioning:\")\n",
    "print(f\"   Total frames: {len(time_windows)}\")\n",
    "print(f\"   Frame duration: {config.BASE_DURATION_US} Œºs\")\n",
    "print(f\"   Frame stride: {stride_us:.1f} Œºs\")\n",
    "print(f\"   Total recording time: {total_recording_time_ms:.2f} ms\")\n",
    "print(f\"   Bluetooth time slots per frame: {config.BASE_DURATION_US / config.BLUETOOTH_SLOT_US:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Load and Process Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_annotations_for_image(img_info: Dict, coco_data: Dict) -> List[BoundingBox]:\n",
    "    \"\"\"\n",
    "    Load and convert COCO annotations to BoundingBox objects\n",
    "    \n",
    "    Args:\n",
    "        img_info: Image information from COCO\n",
    "        coco_data: Complete COCO dataset\n",
    "        \n",
    "    Returns:\n",
    "        List of BoundingBox objects\n",
    "    \"\"\"\n",
    "    img_id = img_info['id']\n",
    "    cat_id_to_name = {cat['id']: cat['name'] for cat in coco_data['categories']}\n",
    "    \n",
    "    img_anns = [ann for ann in coco_data['annotations'] if ann['image_id'] == img_id]\n",
    "    \n",
    "    bboxes = []\n",
    "    for ann in img_anns:\n",
    "        x, y, w, h = ann['bbox']\n",
    "        bboxes.append(BoundingBox(\n",
    "            x=x, y=y, width=w, height=h,\n",
    "            category_id=ann['category_id'],\n",
    "            category_name=cat_id_to_name[ann['category_id']]\n",
    "        ))\n",
    "    \n",
    "    return bboxes\n",
    "\n",
    "# Create image path mapping\n",
    "print(\"\\nüìã Loading annotations...\")\n",
    "img_name_to_path = {img.name: img for img in all_images}\n",
    "for img in all_images:\n",
    "    img_name_to_path[img.stem] = img\n",
    "\n",
    "# Create annotations dictionary\n",
    "annotations_dict = {}\n",
    "for img_info in tqdm(coco_data['images'], desc=\"Processing annotations\"):\n",
    "    img_name = img_info['file_name']\n",
    "    annotations_dict[img_name] = load_annotations_for_image(img_info, coco_data)\n",
    "\n",
    "total_annotations = sum(len(bboxes) for bboxes in annotations_dict.values())\n",
    "print(f\"‚úÖ Annotations loaded: {total_annotations} bounding boxes across {len(annotations_dict)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Run Bluetooth Cutoff Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run sliding window analysis for Bluetooth cutoffs\n",
    "cutoff_analysis = check_bluetooth_cutoff(all_images, annotations_dict, time_windows)\n",
    "\n",
    "# Display sample cutoff detections\n",
    "if cutoff_analysis['detections']:\n",
    "    print(\"\\nüìä Sample cutoff detections:\")\n",
    "    for i, detection in enumerate(cutoff_analysis['detections'][:5]):\n",
    "        print(f\"\\n  Detection {i+1}:\")\n",
    "        print(f\"    Frame: {detection['frame1']} ‚Üí {detection['frame2']}\")\n",
    "        print(f\"    Time: {detection['time_window'].start_us:.1f} - {detection['time_window'].end_us:.1f} Œºs\")\n",
    "        print(f\"    Continues: {'Yes ‚úì' if detection['continues_in_next'] else 'No ‚úó'}\")\n",
    "        print(f\"    Distance from edge: {detection['distance_from_edge']:.1f} px\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_cutoff_detection(detection: Dict, output_path: Optional[Path] = None):\n",
    "    \"\"\"\n",
    "    Visualize a Bluetooth cutoff detection across two frames\n",
    "    \n",
    "    Args:\n",
    "        detection: Detection dictionary from cutoff analysis\n",
    "        output_path: Optional path to save visualization\n",
    "    \"\"\"\n",
    "    img1_path = img_name_to_path.get(detection['frame1'])\n",
    "    img2_path = img_name_to_path.get(detection['frame2'])\n",
    "    \n",
    "    if img1_path is None or img2_path is None:\n",
    "        print(\"‚ö†Ô∏è Could not find image paths\")\n",
    "        return\n",
    "    \n",
    "    img1 = cv2.imread(str(img1_path))\n",
    "    img2 = cv2.imread(str(img2_path))\n",
    "    \n",
    "    if img1 is None or img2 is None:\n",
    "        print(\"‚ö†Ô∏è Could not load images\")\n",
    "        return\n",
    "    \n",
    "    img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "    img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Draw bounding box on img1\n",
    "    bbox = detection['bbox']\n",
    "    color = (0, 255, 0) if detection['continues_in_next'] else (255, 0, 0)\n",
    "    cv2.rectangle(img1, \n",
    "                 (int(bbox.x), int(bbox.y)), \n",
    "                 (int(bbox.x + bbox.width), int(bbox.y + bbox.height)), \n",
    "                 color, 2)\n",
    "    \n",
    "    # Create side-by-side visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    axes[0].imshow(img1)\n",
    "    axes[0].set_title(f\"Frame {detection['frame_idx']}: {detection['frame1']}\\n\"\n",
    "                     f\"Signal at right edge\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(img2)\n",
    "    axes[1].set_title(f\"Frame {detection['frame_idx']+1}: {detection['frame2']}\\n\"\n",
    "                     f\"Continues: {'Yes' if detection['continues_in_next'] else 'No'}\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    status = \"Continuous\" if detection['continues_in_next'] else \"CUT OFF\"\n",
    "    color_text = 'green' if detection['continues_in_next'] else 'red'\n",
    "    fig.suptitle(f\"Bluetooth Signal Analysis: {status}\", fontsize=14, color=color_text, weight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if output_path:\n",
    "        plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"‚úÖ Saved visualization to {output_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Visualize first cutoff detection if available\n",
    "if cutoff_analysis['detections']:\n",
    "    print(\"\\nüñºÔ∏è Visualizing first cutoff detection...\")\n",
    "    visualize_cutoff_detection(cutoff_analysis['detections'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_report():\n",
    "    \"\"\"\n",
    "    Generate comprehensive summary report\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä RF SIGNAL DETECTION - SUMMARY REPORT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nüìÅ DATASET INFORMATION:\")\n",
    "    print(f\"   Location: {config.DATASET_PATH}\")\n",
    "    print(f\"   Total images: {len(all_images)}\")\n",
    "    print(f\"   Total annotations: {total_annotations}\")\n",
    "    \n",
    "    # Count by category\n",
    "    category_counts = {}\n",
    "    for bboxes in annotations_dict.values():\n",
    "        for bbox in bboxes:\n",
    "            category_counts[bbox.category_name] = category_counts.get(bbox.category_name, 0) + 1\n",
    "    \n",
    "    print(\"\\n   Annotations by category:\")\n",
    "    for cat_name, count in sorted(category_counts.items()):\n",
    "        print(f\"     - {cat_name}: {count}\")\n",
    "    \n",
    "    print(\"\\n‚è±Ô∏è TEMPORAL ANALYSIS:\")\n",
    "    print(f\"   Detected temporal overlap: {detected_overlap:.1f}%\")\n",
    "    print(f\"   Frame duration: {config.BASE_DURATION_US} Œºs\")\n",
    "    print(f\"   Frame stride: {stride_us:.1f} Œºs\")\n",
    "    print(f\"   Total recording duration: {total_recording_time_ms:.2f} ms\")\n",
    "    \n",
    "    print(\"\\nüéØ BLUETOOTH CUTOFF ANALYSIS:\")\n",
    "    print(f\"   Signals at boundaries: {cutoff_analysis['total']}\")\n",
    "    print(f\"   Continuous signals: {cutoff_analysis['continuing']}\")\n",
    "    print(f\"   Potentially cut signals: {cutoff_analysis['cut_off']}\")\n",
    "    if cutoff_analysis['total'] > 0:\n",
    "        continuity = 100.0 * cutoff_analysis['continuing'] / cutoff_analysis['total']\n",
    "        print(f\"   Continuity rate: {continuity:.1f}%\")\n",
    "    \n",
    "    print(\"\\nüì° SIGNAL CHARACTERISTICS:\")\n",
    "    print(f\"   Center frequency: {config.CENTER_FREQ_GHZ} GHz\")\n",
    "    print(f\"   Bandwidth: {config.BANDWIDTH_MHZ} MHz\")\n",
    "    print(f\"   Frequency range: {config.CENTER_FREQ_GHZ - config.BANDWIDTH_MHZ/2000:.3f} - \"\n",
    "          f\"{config.CENTER_FREQ_GHZ + config.BANDWIDTH_MHZ/2000:.3f} GHz\")\n",
    "    print(f\"   Bluetooth slots per frame: {config.BASE_DURATION_US / config.BLUETOOTH_SLOT_US:.2f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Generate report\n",
    "generate_summary_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_cutoff_analysis(cutoff_analysis: Dict, output_path: Path):\n",
    "    \"\"\"\n",
    "    Export cutoff analysis results to JSON\n",
    "    \n",
    "    Args:\n",
    "        cutoff_analysis: Analysis results\n",
    "        output_path: Path to save JSON file\n",
    "    \"\"\"\n",
    "    # Convert to serializable format\n",
    "    export_data = {\n",
    "        'summary': {\n",
    "            'total_detections': cutoff_analysis['total'],\n",
    "            'continuing_signals': cutoff_analysis['continuing'],\n",
    "            'cut_off_signals': cutoff_analysis['cut_off'],\n",
    "            'continuity_rate': 100.0 * cutoff_analysis['continuing'] / cutoff_analysis['total'] \n",
    "                              if cutoff_analysis['total'] > 0 else 0.0\n",
    "        },\n",
    "        'detections': []\n",
    "    }\n",
    "    \n",
    "    for det in cutoff_analysis['detections']:\n",
    "        export_data['detections'].append({\n",
    "            'frame_idx': det['frame_idx'],\n",
    "            'frame1': det['frame1'],\n",
    "            'frame2': det['frame2'],\n",
    "            'time_window_start_us': det['time_window'].start_us,\n",
    "            'time_window_end_us': det['time_window'].end_us,\n",
    "            'continues_in_next': det['continues_in_next'],\n",
    "            'distance_from_edge_px': det['distance_from_edge'],\n",
    "            'bbox': {\n",
    "                'x': det['bbox'].x,\n",
    "                'y': det['bbox'].y,\n",
    "                'width': det['bbox'].width,\n",
    "                'height': det['bbox'].height,\n",
    "                'category': det['bbox'].category_name\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(export_data, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Exported cutoff analysis to {output_path}\")\n",
    "\n",
    "# Export results\n",
    "export_path = config.OUTPUT_DIR / 'bluetooth_cutoff_analysis.json'\n",
    "export_cutoff_analysis(cutoff_analysis, export_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "\n",
    "Now that we've analyzed the dataset and checked for Bluetooth signal cutoffs, the next steps are:\n",
    "\n",
    "1. **Implement Channel Slicing** - Split spectrograms into frequency channels\n",
    "2. **Implement Temporal Slicing** - Create overlapping temporal windows\n",
    "3. **Process Annotations** - Transform annotations for sliced images\n",
    "4. **Generate YOLO Dataset** - Create train/val splits with proper formatting\n",
    "5. **Train YOLO Model** - Train object detector on processed dataset\n",
    "\n",
    "These steps can be added as additional cells in this notebook or in a separate continuation notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
